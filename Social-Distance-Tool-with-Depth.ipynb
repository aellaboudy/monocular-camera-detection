{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Perception Demo\n",
    "\n",
    "The Machine Perception tool takes in arbitrary video streams and converts them to metric depth images, while segmenting obstacles and providing physical distances to them.\n",
    "\n",
    "**Input:**\n",
    "- A video sequence\n",
    "- Camera intrinsics\n",
    "- Height of the camera above a ground plane\n",
    "\n",
    "**Output:**\n",
    "- Metric depth maps\n",
    "- Panoptic segmentation of the scene\n",
    "- Bounding boxes with physical distances to anomolies and obstacles in the scene.\n",
    "\n",
    "**Future Development**\n",
    "- Cloud tool for remote monitoring of camera streams and depth + predictions\n",
    "- Automatically fine-tuned models\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "**Import libraries for Detectron2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###!python -m detectron2.utils.collect_env # to check if Detectron2 is working fine\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import open3d as o3d\n",
    "from MiDaS.utils import read_pfm\n",
    "from MiDaS.run import run\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries and files for MonoDepth2 algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for monodepth2\n",
    "from __future__ import absolute_import, division, print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import PIL.Image as pil\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "#import monodepth2.networks as networks\n",
    "#from monodepth2.utils import download_model_if_doesnt_exist\n",
    "#from monodepth2.layers import disp_to_depth\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define key variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_folder = 'frames'\n",
    "result_folder = 'results'\n",
    "depths_folder = 'results/depth'\n",
    "detections_folder = 'results/frames'\n",
    "frame_count = None # Number of frames to consider in the video (use less for faster calculations) # None will take all frames\n",
    "img_ext = 'png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Video to PNG Frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘frames/’: File exists\n",
      "mkdir: cannot create directory ‘results/frames/’: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:01, 19.39it/s]\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $frames_folder/*\n",
    "!mkdir $frames_folder/\n",
    "!rm -rf $detections_folder/*\n",
    "!mkdir $detections_folder/\n",
    "\n",
    "#specify path to video\n",
    "video = \"output.mp4\"\n",
    "\n",
    "#capture video\n",
    "cap = cv2.VideoCapture(video)\n",
    "cnt=0\n",
    "FPS=cap.get(cv2.CAP_PROP_FPS)\n",
    "freq = 0.1 #Num of desired seconds\n",
    "# Check if video file is opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    "\n",
    "ret,first_frame = cap.read()\n",
    "\n",
    "#Read until video is completed\n",
    "with tqdm(total=frame_count) as pbar:\n",
    "    while(cap.isOpened()):\n",
    "\n",
    "      # Capture frame-by-frame\n",
    "      ret, frame = cap.read()\n",
    "      pbar.update(1)\n",
    "      if ret == True:\n",
    "\n",
    "        #save each frame to folder        \n",
    "        cv2.imwrite(frames_folder+'/{:04d}'.format(cnt)+'.png', frame)\n",
    "        cnt=int(cnt+FPS*freq)\n",
    "        cap.set(1,cnt)\n",
    "        if(cnt==frame_count) and frame_count != None:\n",
    "          break\n",
    "      # Break the loop\n",
    "      else: \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading MonoDept2 pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_name = \"mono+stereo_640x192\"\\ndownload_model_if_doesnt_exist(model_name)\\nencoder_path = os.path.join(\"monodepth2/models\", model_name, \"encoder.pth\")\\ndepth_decoder_path = os.path.join(\"monodepth2/models\", model_name, \"depth.pth\")\\n\\n\\n# LOADING PRETRAINED MODEL\\nencoder = networks.ResnetEncoder(18, False)\\nloaded_dict_enc = torch.load(encoder_path, map_location=device)\\nfiltered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\\nencoder.load_state_dict(filtered_dict_enc)\\n\\n# extract the height and width of image that this model was trained with\\nfeed_height = loaded_dict_enc[\\'height\\']\\nfeed_width = loaded_dict_enc[\\'width\\']\\n\\n\\nencoder.to(device)\\nencoder.eval();\\n\\n# LOADING PRETRAINED MODEL\\ndepth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\\nloaded_dict = torch.load(depth_decoder_path, map_location=device)\\ndepth_decoder.load_state_dict(loaded_dict)\\n\\ndepth_decoder.to(device)\\ndepth_decoder.eval();\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available(): # and not args.no_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\"\"\"\n",
    "model_name = \"mono+stereo_640x192\"\n",
    "download_model_if_doesnt_exist(model_name)\n",
    "encoder_path = os.path.join(\"monodepth2/models\", model_name, \"encoder.pth\")\n",
    "depth_decoder_path = os.path.join(\"monodepth2/models\", model_name, \"depth.pth\")\n",
    "\n",
    "\n",
    "# LOADING PRETRAINED MODEL\n",
    "encoder = networks.ResnetEncoder(18, False)\n",
    "loaded_dict_enc = torch.load(encoder_path, map_location=device)\n",
    "filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "encoder.load_state_dict(filtered_dict_enc)\n",
    "\n",
    "# extract the height and width of image that this model was trained with\n",
    "feed_height = loaded_dict_enc['height']\n",
    "feed_width = loaded_dict_enc['width']\n",
    "\n",
    "\n",
    "encoder.to(device)\n",
    "encoder.eval();\n",
    "\n",
    "# LOADING PRETRAINED MODEL\n",
    "depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
    "loaded_dict = torch.load(depth_decoder_path, map_location=device)\n",
    "depth_decoder.load_state_dict(loaded_dict)\n",
    "\n",
    "depth_decoder.to(device)\n",
    "depth_decoder.eval();\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing depth estimation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDepth(image_path,output_directory,ext):\n",
    "    # FINDING INPUT IMAGES\n",
    "    if os.path.isfile(image_path):\n",
    "        # Only testing on a single image\n",
    "        paths = [image_path]\n",
    "        #output_directory = os.path.dirname(args.image_path)\n",
    "    elif os.path.isdir(image_path):\n",
    "        # Searching folder for images\n",
    "        paths = glob.glob(os.path.join(image_path, '*.{}'.format(ext)))\n",
    "        #output_directory = args.image_path\n",
    "    else:\n",
    "        raise Exception(\"Can not find args.image_path: {}\".format(image_path))\n",
    "    #print(\"-> Predicting on {:d} test images\".format(len(paths)))\n",
    "\n",
    "\n",
    "    # PREDICTING ON EACH IMAGE IN TURN\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(paths)) as pbar:\n",
    "            for idx, image_path in (enumerate(paths)):\n",
    "\n",
    "                if image_path.endswith(\"_disp.jpg\"):\n",
    "                    # don't try to predict disparity for a disparity image!\n",
    "                    continue\n",
    "\n",
    "                # Load image and preprocess\n",
    "                input_image = pil.open(image_path).convert('RGB')\n",
    "                original_width, original_height = input_image.size\n",
    "                input_image = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "                input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n",
    "\n",
    "                # PREDICTION\n",
    "                input_image = input_image.to(device)\n",
    "                features = encoder(input_image)\n",
    "                outputs = depth_decoder(features)\n",
    "\n",
    "                disp = outputs[(\"disp\", 0)]\n",
    "                disp_resized = torch.nn.functional.interpolate(\n",
    "                    disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "                print(\"Output of network resized:\")\n",
    "                print(np.shape(disp_resized))\n",
    "                # Saving numpy file\n",
    "                output_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "                name_dest_npy = os.path.join(output_directory, \"{}_disp.npy\".format(output_name))\n",
    "                _, scaled_disp = disp_to_depth(disp, 3.0, 5000)\n",
    "                np.save(name_dest_npy, scaled_disp.cpu().numpy())\n",
    "                \n",
    "                print(\"Absolute depth size:\")\n",
    "                print(np.shape(scaled_disp))\n",
    "                #print(np.shape(disp))\n",
    "                #break\n",
    "                \n",
    "                # Saving colormapped depth image\n",
    "                disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "                \n",
    "                #print(disp_resized_np)\n",
    "                \n",
    "                vmax = np.percentile(disp_resized_np, 95)\n",
    "                normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=vmax)\n",
    "                \n",
    "                mapper = cm.ScalarMappable(norm=normalizer, cmap='magma')\n",
    "                colormapped_im = (mapper.to_rgba(disp_resized_np)[:, :, :3] * 255).astype(np.uint8)\n",
    "                \n",
    "                #print(np.shape(colormapped_im))\n",
    "                #break\n",
    "                \n",
    "                im = pil.fromarray(colormapped_im)\n",
    "                \n",
    "                name_dest_im = os.path.join(output_directory, \"{}_disp.jpeg\".format(output_name))\n",
    "                im.save(name_dest_im)\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘results/depth’: File exists\n",
      "initialize\n",
      "device: cuda\n",
      "Loading weights:  ./MiDaS/model-f6b98070.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/awsgui/.cache/torch/hub/facebookresearch_WSL-Images_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing\n",
      "  processing frames/0018.png (1/22)\n",
      "  processing frames/0010.png (2/22)\n",
      "  processing frames/0000.png (3/22)\n",
      "  processing frames/0016.png (4/22)\n",
      "  processing frames/0006.png (5/22)\n",
      "  processing frames/0007.png (6/22)\n",
      "  processing frames/0020.png (7/22)\n",
      "  processing frames/0008.png (8/22)\n",
      "  processing frames/0015.png (9/22)\n",
      "  processing frames/0009.png (10/22)\n",
      "  processing frames/0005.png (11/22)\n",
      "  processing frames/0001.png (12/22)\n",
      "  processing frames/0021.png (13/22)\n",
      "  processing frames/0003.png (14/22)\n",
      "  processing frames/0002.png (15/22)\n",
      "  processing frames/0013.png (16/22)\n",
      "  processing frames/0019.png (17/22)\n",
      "  processing frames/0012.png (18/22)\n",
      "  processing frames/0014.png (19/22)\n",
      "  processing frames/0004.png (20/22)\n",
      "  processing frames/0017.png (21/22)\n",
      "  processing frames/0011.png (22/22)\n",
      "finished\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.00x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "38.147606110234925\n",
      "B Factor:\n",
      "2.0816681711721685e-17\n",
      "-3.8147604\n",
      "58878580.0\n",
      "-1.0629554\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.00x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "38.02128601852626\n",
      "B Factor:\n",
      "6.938893903907228e-18\n",
      "-3.8021286\n",
      "58683612.0\n",
      "-1.0594356\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.01x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "36.9628907483419\n",
      "B Factor:\n",
      "-6.938893903907228e-18\n",
      "-3.696289\n",
      "58033508.0\n",
      "-1.0033295\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: 0.01x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "38.834435995383146\n",
      "B Factor:\n",
      "1.3877787807814457e-17\n",
      "-3.8834434\n",
      "58594144.0\n",
      "-1.0860205\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.02x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "37.48654378033657\n",
      "B Factor:\n",
      "1.3877787807814457e-17\n",
      "-3.7486546\n",
      "57552656.0\n",
      "-1.0483263\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.05x + 0.91y + 0.40z + -0.05 = 0\n",
      "Scale factor:\n",
      "36.18177241546392\n",
      "B Factor:\n",
      "6.938893903907228e-18\n",
      "-3.6181774\n",
      "63011220.0\n",
      "-1.0118377\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.03x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "35.542788132169505\n",
      "B Factor:\n",
      "-1.3877787807814457e-17\n",
      "-3.554279\n",
      "63345824.0\n",
      "-0.9939684\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: 0.00x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "37.331553249518414\n",
      "B Factor:\n",
      "-6.938893903907228e-18\n",
      "-3.7331555\n",
      "59472960.0\n",
      "-1.0439919\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: 0.00x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "38.16593129780145\n",
      "B Factor:\n",
      "0.0\n",
      "-3.8165932\n",
      "53272356.0\n",
      "-1.0673256\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: 0.00x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "36.95376198962149\n",
      "B Factor:\n",
      "-6.938893903907228e-18\n",
      "-3.6953762\n",
      "56336704.0\n",
      "-1.0334268\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.01x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "38.049124805124926\n",
      "B Factor:\n",
      "6.938893903907228e-18\n",
      "-3.8049126\n",
      "57399824.0\n",
      "-1.064059\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.01x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "37.64637762940238\n",
      "B Factor:\n",
      "0.0\n",
      "-3.7646377\n",
      "53928310.0\n",
      "-1.052796\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.01x + 0.91y + 0.42z + -0.05 = 0\n",
      "Scale factor:\n",
      "40.13831976808539\n",
      "B Factor:\n",
      "6.938893903907228e-18\n",
      "-4.013832\n",
      "54849696.0\n",
      "-1.1224842\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.00x + 0.91y + 0.42z + -0.05 = 0\n",
      "Scale factor:\n",
      "39.48324134804975\n",
      "B Factor:\n",
      "-1.3877787807814457e-17\n",
      "-3.9483242\n",
      "59706292.0\n",
      "-1.1041646\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: 0.00x + 0.91y + 0.42z + -0.05 = 0\n",
      "Scale factor:\n",
      "39.21610833363575\n",
      "B Factor:\n",
      "-6.938893903907228e-18\n",
      "-3.921611\n",
      "56051668.0\n",
      "-1.0924102\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.01x + 0.91y + 0.41z + -0.04 = 0\n",
      "Scale factor:\n",
      "40.93443745799205\n",
      "B Factor:\n",
      "0.0\n",
      "-4.0934434\n",
      "53196172.0\n",
      "-1.1447477\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.01x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "39.109545117221636\n",
      "B Factor:\n",
      "1.3877787807814457e-17\n",
      "-3.9109545\n",
      "55845492.0\n",
      "-1.0921153\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.00x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "40.56261049447288\n",
      "B Factor:\n",
      "6.938893903907228e-18\n",
      "-4.056261\n",
      "54909428.0\n",
      "-1.1343496\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.03x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "40.31764289263131\n",
      "B Factor:\n",
      "0.0\n",
      "-4.031764\n",
      "54274656.0\n",
      "-1.1274989\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.01x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "40.52502893681025\n",
      "B Factor:\n",
      "1.3877787807814457e-17\n",
      "-4.052503\n",
      "56795148.0\n",
      "-1.1332989\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.01x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "40.45620503987998\n",
      "B Factor:\n",
      "0.0\n",
      "-4.0456204\n",
      "52868852.0\n",
      "-1.131374\n",
      "[[440.692   0.    354.835]\n",
      " [  0.    440.692 232.086]\n",
      " [  0.      0.      1.   ]]\n",
      "Plane equation: -0.02x + 0.91y + 0.41z + -0.05 = 0\n",
      "Scale factor:\n",
      "40.52571676169192\n",
      "B Factor:\n",
      "0.0\n",
      "-4.0525713\n",
      "52127004.0\n",
      "-1.1320922\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $depths_folder/*\n",
    "!mkdir $depths_folder\n",
    "run(frames_folder,depths_folder,'./MiDaS/model-f6b98070.pt')\n",
    "intrinsic = o3d.io.read_pinhole_camera_intrinsic(\"intrinsicsApple.json\")\n",
    "\n",
    "c_imgs = glob.glob(frames_folder+\"/*.png\")\n",
    "c_imgs.sort()\n",
    "\n",
    "d_imgs = glob.glob(depths_folder+\"/*.pfm\")\n",
    "d_imgs.sort()\n",
    "\n",
    "h = 2.0\n",
    "\n",
    "for idx in range(len(c_imgs)):\n",
    "    color = o3d.io.read_image(c_imgs[idx])\n",
    "    idepth = read_pfm(d_imgs[idx])[0]\n",
    "    idepth = idepth - np.amin(idepth)\n",
    "    idepth[idepth < 0.0001] = 0.0001\n",
    "    idepth /= np.amax(idepth)\n",
    "    idepth *= 10\n",
    "\n",
    "    focal = intrinsic.intrinsic_matrix[0, 0]\n",
    "    print(intrinsic.intrinsic_matrix)\n",
    "    depth = focal / (idepth)\n",
    "    #depth[depth >= 100 * focal] = np.inf\n",
    "    #print(\"Depth Shape\")\n",
    "    #print(depth.shape)\n",
    "    depth[:,:int(depth.shape[1]/4)] = -100\n",
    "    depth[:,int(3*depth.shape[1]/4):] = -100\n",
    "    imgDepth = o3d.geometry.Image(depth)\n",
    "\n",
    "    rgbdi = o3d.geometry.RGBDImage.create_from_color_and_depth(color, imgDepth)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbdi, intrinsic)\n",
    "    plane_model, inliers = pcd.segment_plane(distance_threshold=0.1,ransac_n=3,num_iterations=100)\n",
    "    [a, b, c, d] = plane_model\n",
    "    print(f\"Plane equation: {a:.2f}x + {b:.2f}y + {c:.2f}z + {d:.2f} = 0\")\n",
    "    m = (d/b - d/c) / (h*(b/c -1))\n",
    "    g = -m*h - d/b\n",
    "    m = -h*b/d\n",
    "    print (\"Scale factor:\")\n",
    "    print(m)\n",
    "    print(\"B Factor:\")\n",
    "    print(g)\n",
    "    \n",
    "    \"\"\"\n",
    "    depth_min = depth(478,373)\n",
    "    print(np.unravel_index(np.argmin(depth, axis=None), depth.shape))\n",
    "    print (depth_min)\n",
    "    m = h/depth_min\n",
    "    print (\"Scale factor:\")\n",
    "    print(m)\n",
    "    \"\"\"\n",
    "    \n",
    "    depth = depth*m/1000.0\n",
    "    print (np.amin(depth))\n",
    "    print (np.amax(depth))\n",
    "    print (np.median(depth))\n",
    "    np.save(depths_folder+f\"/{idx}.npy\", depth)\n",
    "    #o3d.io.write_point_cloud(f\"{idx}.npy\", pcd)\n",
    "\n",
    "\n",
    "#findDepth(frames_folder,depths_folder,img_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download a pretrained model from Detectron2 Model Zoo for Faster-RCNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3 # 3 classes (data, fig, hazelnut)\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(\"/home/awsgui/voc2coco/output/\", \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8\n",
    "#cfg.MODEL.DEVICE ='cpu'\n",
    "predictor = DefaultPredictor(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all the key functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_update(mean1, var1, mean2, var2):\n",
    "    kalman_gain = 0.2 #Gain for new measurements\n",
    "    if mean1 < 0 or var1 < 0:\n",
    "        mean1 = mean2\n",
    "        var1 = var2\n",
    "    Kalmansum = var1*kalman_gain + var2*(1 - kalman_gain)\n",
    "    pr_s = mean1*var2*(1 - kalman_gain) + mean2*var1*kalman_gain\n",
    "    new_mean = pr_s/Kalmansum\n",
    "    product = var1*var2\n",
    "    new_var = product/Kalmansum\n",
    "    return new_mean,new_var\n",
    "\n",
    "\n",
    "# define a function which return the bottom center of every bbox\n",
    "z_array = [0,0,0,0,0]\n",
    "z_estimate= [-1,-1]\n",
    "\n",
    "def mid_point(img,img_depth,person,idx):\n",
    "  #get the coordinates\n",
    "  x1,y1,x2,y2 = person[idx]\n",
    "  _ = cv2.rectangle(img, (x1, y1), (x2, y2), (0,0,255), 2)\n",
    "  \n",
    "  #compute bottom center of bbox\n",
    "  x_mid = int((x1+x2)/2)\n",
    "  y_mid = int(y2)\n",
    "  mid   = (x_mid-1,y_mid-1)\n",
    "  print(\"mid=\",mid)\n",
    "  print(\"img_depth\",img_depth.shape)\n",
    "  z_mid = img_depth[(y_mid-1,x_mid-1)]\n",
    "  for i in range(0,4):\n",
    "      z_array[i] = z_array[i+1]\n",
    "  z_array[4] = z_mid\n",
    "  #z_mid = np.median(z_array)\n",
    "  z_estimate[0], z_estimate[1] = kalman_update(z_estimate[0], z_estimate[1], z_mid, 2.0)\n",
    "  z_mid = z_estimate[0]\n",
    "  mid3d = (x_mid,y_mid,z_mid)\n",
    "    \n",
    "  _ = cv2.circle(img, mid, 5, (0, 0, 255), -1)\n",
    "  cv2.putText(img, str(z_mid) + \" m\", mid, cv2.FONT_HERSHEY_SIMPLEX,1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "  return mid3d\n",
    "\n",
    "# define a function which computes euclidean distance between two midpoints\n",
    "from scipy.spatial import distance\n",
    "def compute_distance(midpoints,num):\n",
    "  dist = np.zeros((num,num))\n",
    "  for i in range(num):\n",
    "    for j in range(i+1,num):\n",
    "      if i!=j:\n",
    "        dst = distance.euclidean(midpoints[i], midpoints[j])\n",
    "        dist[i][j]=dst\n",
    "  return dist\n",
    "\n",
    "\n",
    "# Finds pairs of people who are close together\n",
    "def find_closest(dist,num,thresh):\n",
    "  p1=[]\n",
    "  p2=[]\n",
    "  d=[]\n",
    "  for i in range(num):\n",
    "    for j in range(i,num):\n",
    "      if( (i!=j) & (dist[i][j]<=thresh)):\n",
    "        p1.append(i)\n",
    "        p2.append(j)\n",
    "        d.append(dist[i][j])\n",
    "  return p1,p2,d\n",
    "\n",
    "\n",
    "# Given pairs of people who are close, color them red\n",
    "def change_2_red(img,img_depth,person,p1,p2):\n",
    "  mid1 = []\n",
    "  mid2 = []\n",
    "  for p in p1:\n",
    "    mid1.append(mid_point(img,img_depth,person,p))\n",
    "  for pp in p2:\n",
    "    mid2.append(mid_point(img,img_depth,person,pp))\n",
    "  for inx in range(len(mid1)):\n",
    "      #print(\"mid1\",mid1[inx][:2])\n",
    "      _ = cv2.line(img, mid1[inx][:2], mid2[inx][:2], (0,255,0), thickness=2, lineType=8, shift=0)\n",
    "  \n",
    "  risky = np.unique(p1+p2)\n",
    "  for i in risky:\n",
    "    x1,y1,x2,y2 = person[i]\n",
    "    _ = cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,0), 2)  \n",
    "  return img\n",
    "\n",
    "\n",
    "# Main function to find closest people\n",
    "def find_closest_people(name,name_depth,thresh,savedir):\n",
    "\n",
    "  img = cv2.imread(name)\n",
    "  depth = np.load(name_depth)\n",
    "  print(\"NPY file shape\")\n",
    "  print(depth.shape)\n",
    "  original_height, original_width,_ = img.shape # (1920,1080) #input_image.size\n",
    "  print(\"Original width, original_height:\")\n",
    "  print(original_width)\n",
    "  print(original_height)\n",
    "  disp_resized = torch.nn.functional.interpolate(\n",
    "                    torch.from_numpy(depth).unsqueeze(0).unsqueeze(0), (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "  img_depth = disp_resized.squeeze().cpu().numpy()\n",
    "\n",
    "  print(\"Depth resized shape, input to algorithms\")\n",
    "  print(disp_resized.shape)\n",
    "  outputs = predictor(img)\n",
    "  classes=outputs['instances'].pred_classes.cpu().numpy()\n",
    "  bbox=outputs['instances'].pred_boxes.tensor.cpu().numpy()\n",
    "  ind = np.where(classes < 2)[0]\n",
    "  person=bbox[ind]\n",
    "  midpoints = [mid_point(img,img_depth,person,i) for i in range(len(person))]\n",
    "  #num = len(midpoints)\n",
    "  #dist= compute_distance(midpoints,num)\n",
    "  #p1,p2,d=find_closest(dist,num,thresh)\n",
    "  #img = change_2_red(img,img_depth,person,p1,p2)\n",
    "  cv2.imwrite(savedir+'/'+name,img)\n",
    "  #print(savedir+'/'+name)\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch all the frames of the video sequence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[]\n",
    "for file in os.listdir(frames_folder):\n",
    "    if file.endswith(\".png\"):\n",
    "        frames.append(frames_folder+\"/\"+file)\n",
    "frames.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch all the frame depths of the video sequence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_depths=[]\n",
    "for file in os.listdir(depths_folder):\n",
    "    if file.endswith(\".npy\"):\n",
    "        frame_depths.append(depths_folder+\"/\"+file)\n",
    "frame_depths.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop to get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]/home/awsgui/detectron2/detectron2/modeling/roi_heads/fast_rcnn.py:154: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  filter_inds = filter_mask.nonzero()\n",
      "  5%|▍         | 1/22 [00:00<00:04,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (295, 86)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3/22 [00:00<00:03,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid= (295, 86)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (297, 91)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5/22 [00:00<00:02,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid= (300, 98)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (301, 104)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7/22 [00:01<00:02,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid= (302, 107)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (307, 111)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 9/22 [00:01<00:02,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid= (311, 114)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (319, 121)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 11/22 [00:01<00:01,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid= (325, 127)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (331, 126)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 13/22 [00:02<00:01,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid= (345, 127)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (353, 131)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 15/22 [00:02<00:01,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid= (366, 136)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (382, 138)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 17/22 [00:02<00:00,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid= (395, 142)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (408, 144)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 18/22 [00:02<00:00,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (423, 146)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 20/22 [00:03<00:00,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid= (436, 145)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (455, 145)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:03<00:00,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid= (472, 147)\n",
      "img_depth (480, 720)\n",
      "NPY file shape\n",
      "(480, 720)\n",
      "Original width, original_height:\n",
      "720\n",
      "480\n",
      "Depth resized shape, input to algorithms\n",
      "torch.Size([1, 1, 480, 720])\n",
      "mid= (499, 112)\n",
      "img_depth (480, 720)\n",
      "mid= (489, 151)\n",
      "img_depth (480, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#from tqdm import tqdm\n",
    "thresh=110\n",
    "with tqdm(total=len(frames)) as pbar:\n",
    "    for i in range(len(frames)):\n",
    "        find_closest_people(frames[i],frame_depths[i],thresh,result_folder)\n",
    "        pbar.update(1)\n",
    "    \n",
    "#_ = [find_closest_people(frames[i],thresh,'frames2') for i in tqdm(range(len(frames))) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main file with highlighed results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 92.98it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 198.68it/s]\n"
     ]
    }
   ],
   "source": [
    "frames=[]\n",
    "for file in os.listdir(result_folder+\"/frames/\"):\n",
    "    if file.endswith(\".png\"):\n",
    "        frames.append(file)\n",
    "frames.sort()\n",
    "\n",
    "frame_array=[]\n",
    "with tqdm(total=len(frames)) as pbar:\n",
    "    for i in range(len(frames)):\n",
    "\n",
    "        #reading each files\n",
    "        img = cv2.imread(result_folder+'/frames/'+frames[i])\n",
    "        #img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        height, width, layers = img.shape\n",
    "        size = (width,height)\n",
    "\n",
    "        #inserting the frames into an image array\n",
    "        frame_array.append(img)\n",
    "        pbar.update(1)\n",
    "\n",
    "out = cv2.VideoWriter(result_folder+'/result.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 10, size)\n",
    " \n",
    "with tqdm(total=len(frames)) as pbar:\n",
    "    for i in range(len(frame_array)):\n",
    "        # writing to a image array\n",
    "        out.write(frame_array[i])\n",
    "        pbar.update(1)\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Depth map video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 127.66it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 376.56it/s]\n"
     ]
    }
   ],
   "source": [
    "frames_depth=[]\n",
    "for file in os.listdir(depths_folder):\n",
    "    if file.endswith(\".png\"):\n",
    "        frames_depth.append(file)\n",
    "frames_depth.sort()\n",
    "\n",
    "frame_array=[]\n",
    "with tqdm(total=len(frames_depth)) as pbar:\n",
    "    for i in range(len(frames_depth)):\n",
    "\n",
    "        #reading each files\n",
    "        img = cv2.imread(depths_folder+'/'+frames_depth[i])\n",
    "        #img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "\n",
    "        height, width, layers = img.shape\n",
    "        size = (width,height)\n",
    "\n",
    "        #inserting the frames into an image array\n",
    "        frame_array.append(img)\n",
    "        pbar.update(1)\n",
    "\n",
    "out = cv2.VideoWriter(result_folder+'/depth_result.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 10, size)\n",
    " \n",
    "with tqdm(total=len(frames_depth)) as pbar:\n",
    "    for i in range(len(frame_array)):\n",
    "        # writing to a image array\n",
    "        out.write(frame_array[i])\n",
    "        pbar.update(1)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
